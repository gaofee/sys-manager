server.port=9001
#数据库连接
spring.datasource.driver-class-name=com.mysql.jdbc.Driver
spring.datasource.username=root
spring.datasource.password=gaofei
spring.datasource.url=jdbc:mysql://10.39.25.109:3306/sys_permission?useSSL=false&useUnicode=true&characterEncoding=UTF-8&useJDBCCompliantTimezoneShift=true&useLegacyDatetimeCode=false&serverTimezone=UTC


#springmvc 配置
#spring.mvc.view.prefix=/WEB-INF/jsp/
#spring.mvc.view.suffix=.jsp

#freemark 的配置
#spring.freemarker.charset=UTF-8
#spring.freemarker.suffix=.ftl
#spring.freemarker.content-type=text/html; charset=utf-8
#spring.freemarker.template-loader-path=classpath:/templates
#spring.mvc.static-path-pattern=/static/**

#文件上传
#=============文件上传
#静态资源对外暴露的访问路径
file.staticAccessPath=/upload/**
#文件上传目录
file.uploadFolder=D:/upload/

##配置路径映射
web.upload-path=D:/**
spring.resources.static-locations=classpath:/META-INF/resources/,classpath:/resources/,classpath:/static/,classpath:/public/,file:${web.upload-path}
#文件大小设置
spring.servlet.multipart.enabled=true
spring.servlet.multipart.max-file-size=30MB
spring.servlet.multipart.max-request-size=100MB

#分页插件
pagehelper.helper-dialect=mysql
pagehelper.params=count=countSql
pagehelper.reasonable=true
pagehelper.support-methods-arguments=true

#mybatis
mybatis.mapper-locations=classpath:mapper/*.xml
mybatis.type-aliases-package=com.gaofei.permission.domain
#mybatis-plus
mybatis-plus.mapper-locations=classpath:mappers/*.xml
mybatis-plus.type-aliases-package=com.gaofei.permission.domain
#打印sql
mybatis-plus.configuration.log-impl=org.apache.ibatis.logging.stdout.StdOutImpl

#配置自定义starter的文件上传路径
file.domain=http://localhost:${server.port}/upload/
file.enable=true
#file.path=D:/upload/
file.path=/opt/

#redis
spring.redis.host=localhost
spring.redis.port=6379

#============== kafka ===================
# 指定kafka 代理地址，可以多个
#spring.kafka.bootstrap-servers=192.168.21.138:9092


#
##=============== provider  =======================
#此处是生产者发送消息不丢失:
#设置为0，producer不进行消息发送的确认，kafka server可能由于一些原因并没有收到对应消息，从而引起消息丢失。
#设置为1，producer在确认到 topic leader 已经接收到消息后，完成发送，此时有可能 follower 并没有接收到对应消息。此时如果 leader 突然宕机，在经过选举之后，没有接到消息的 follower 晋升为 leader，从而引起消息丢失。
#设置为-1可能很好的确认kafka server是否已经完成消息的接收和本地化存储，并且可以在producer发送失败时进行重试。

# 0:表示不进行消息接收是否成功的确认
#1：表示当Leader接收成功时确认
#-1：表示Leader和Follower都接收成功时确认
#spring.kafka.producer.acks=1



#spring.kafka.producer.retries=0
## 每次批量发送消息的数量
#spring.kafka.producer.batch-size=16384
#spring.kafka.producer.buffer-memory=33554432
#
## 指定消息key和消息体的编解码方式
#spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
#spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
#
##=============== consumer  =======================
#此处是消费者发送消息不丢失:
# 消费方丢失的情况，是因为在消费过程中出现了异常，但是 对应消息的 offset 已经提交，那么消费异常的消息将会丢失。
#    offset的提交包括手动提交和自动提交，可通过kafka.consumer.enable-auto-commit进行配置。
#    手动提交可以灵活的确认是否将本次消费数据的offset进行提交，可以很好的避免消息丢失的情况。
#    自动提交是引起数据丢失的主要诱因。因为消息的消费并不会影响到offset的提交。
#关闭自动提交
#spring.kafka.consumer.enable-auto-commit=false



## 指定默认消费者group id
#spring.kafka.consumer.group-id=test-consumer-group
#
#spring.kafka.consumer.auto-offset-reset=earliest
#spring.kafka.consumer.enable-auto-commit=true
#spring.kafka.consumer.auto-commit-interval=100
#
## 指定消息key和消息体的编解码方式
#spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#
#
##ElasticSearch 配置//名字必须和elasticsearch.yml里面的cluster.name相同
#spring.data.elasticsearch.cluster-name=my-application
#spring.data.elasticsearch.cluster-nodes=localhost:9300
#spring.data.elasticsearch.repositories.enabled=true


#email 的配置
#你自己的邮箱,用来给别人发邮件的
spring.mail.host=smtp.qq.com
spring.mail.username=1813863124@qq.com
spring.mail.password=uecxevtvvitvdedi
spring.mail.default-encoding=utf-8



